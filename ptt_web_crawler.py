# -*- coding: utf-8 -*-
"""人文程設 PTT 爬蟲.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J8pZQgO_ZJJO7LzCOmffTbfHvwQt1vB8

匯入所需套件  
(中文斷詞jieba、製作文字雲套件、使用者介面設定套件....)
"""

import requests   #抓網址 
import re    #正則表達式（抓code 規律）
import jieba #斷詞處理
import matplotlib.pyplot as plt     #畫畫 處理圖片
import numpy as np  #快速匯出圖片 
from PIL import Image   #開圖片
from urllib.parse import urlparse, parse_qs, quote, urljoin   #處理網址
from bs4 import BeautifulSoup  #解析網址
from wordcloud import WordCloud  #畫文字雲
from collections import Counter     #數詞彙種類
from kivy.app import App
from kivy.uix.gridlayout import GridLayout
from kivy.uix.textinput import TextInput
from kivy.uix.button import Button
from kivy.uix.label import Label
from kivy.uix.popup import Popup

"""開始爬蟲"""

#PTT爬蟲主要程式
class PTTCrawler:
    def __init__(self): #告訴程式我們要爬的網站
        self.base_url = "https://www.ptt.cc/"    

    def crawl(self, board, pages):        #本體
        url = f"https://www.ptt.cc/bbs/{board}/index.html"  #board 可以根據input改變
        res = requests.get(url, cookies={'over18': '1'}).text     #處理是否滿18
        soup = BeautifulSoup(res, 'lxml')
        total_contents = []  #存每一頁的東西（有每一頁的list)
        for i in range(pages):      #page:一個page有十個文章,article文章  （每一個回圈都抓一頁一頁）
            # list of list of paragraph
            # [[str], [str], [str]]
            page_contents = self.grab_a_page(url)   
            # list of list of list of paragraph
            # [[[str], [str], [str]], [[str], [str], [str]], [[str], [str], [str]]]
            total_contents.append(page_contents)  #將每一頁加進list中
            controls = soup.findAll("a", {"class": "btn wide"})   #按上一頁，再做一次
            link = controls[1].get('href')
            url = urljoin('https://www.ptt.cc/', link)
            print(f"{i + 1} / {pages} pages done")
        print("Analyzing content......")
        stacked = [[word for word in jieba.cut(content)] for page_content in total_contents for content in page_content]  #斷詞   
        unpacked = [word for s in stacked for word in s] #把list去掉，變成全部都是文字(將list中的小list合併)
        stop_words = self.read_stop_word_file("stop_words.txt")   #去除stop words
        unpacked = [word for word in unpacked if word not in stop_words]
        my_counter = Counter(unpacked)     #counter：詞語為key，頻率為value   #把詞語斷好，頻率數給你
        return my_counter

    @staticmethod
    #建立無意義文字檔案
    def read_stop_word_file(file):
        stop_words = [line.strip() for line in open(file, 'r', encoding='utf-8').readlines()] 
        return stop_words
    #製作文字雲
    def generate_cloud(self, count_dict):   #文字雲 （接收數數結果，吐一張雲給你）
        # font = '/System/Library/Fonts/STHeiti Medium.ttc'
        print(count_dict)
        # ntu_mask = np.array(Image.open("ntu.jpeg"))
        mouth_mask = np.array(Image.open("mouth.jpg"))
        font = 'STHeiti Medium.ttc'
        # my_wordcloud = WordCloud(font_path=font, mask=ntu_mask)
        my_wordcloud = WordCloud(font_path=font, mask=mouth_mask)
        my_wordcloud.generate_from_frequencies(frequencies=count_dict)
        plt.imshow(my_wordcloud)
        plt.axis("off")
        plt.show()

    #抓PTT每一頁的文章
    def grab_a_page(self, url):   #抓每一頁的文章  
        res = requests.get(url, cookies={'over18': '1'}).text
        soup = BeautifulSoup(res, 'lxml')
        contents = []    #contents會被裝到total content 裡面 （裡面有很多文章）一頁，裡面有十個文章
        for s in soup.findAll("div", {'class': "title"}):   #點進去標題
            try:
                content_url = self.base_url + s.a['href']    #抓每個標題的url
                content = self.grab_an_article(content_url)   #回傳文章內容
                contents.append(content)
            except Exception:
                pass
        return contents

    @staticmethod
    #抓每一篇文章的內容
    def grab_an_article(url):    #(找規律)找到標題url，把文章回傳
        res = requests.get(url, cookies={'over18': '1'}).text
        soup = BeautifulSoup(res, 'lxml')
        content = soup.find(id="main-content")    #怎麼抓
        filtered = [v for v in content.stripped_strings if v[0] not in [u'※', u'◆'] and v[:2] not in [u'--']]  #去除符號
        end_str = 'https://www.ptt'
        end_idx = 0
        for idx, sentence in enumerate(filtered):
            if end_str in sentence:
                end_idx = idx
                break
        article = ' '.join(filtered[8:end_idx])
        article = re.sub(r'(\s)+', '', article)
        article = re.sub(r'https?://[a-z0-9.A-Z:\/%]+', '', article)
        return article

"""使用者介面設定"""

#建立使用者介面
class UserInterface(GridLayout):
    def __init__(self):      #佈置哪個東西放哪裡
        super().__init__()
        self.cols = 2      #使用者介面共2行3列
        self.rows = 3
        self.add_widget(Label(text="Please enter board:", font_size=40)) #使用者輸入想要爬的板
        self.board = TextInput(multiline=False, font_size=40)
        self.add_widget(self.board)
        self.add_widget(Label(text="Please enter amount:", font_size=40)) #使用者輸入想要爬幾篇文章
        self.amount = TextInput(multiline=False, font_size=40)
        self.add_widget(self.amount)
        self.run = Button(text='Run', font_size=55)
        self.run.bind(on_press=self._run_crawler)
        self.add_widget(Label())
        self.add_widget(self.run)
        self.pop_up = Popup(title='Waring',
                            content=Label(text='Input amount must be integer'),
                            size_hint=(None, None),
                            size=(400, 400))

    def _run_crawler(self, instance):    #把東西爬完，文字雲照片生出來 
        try:
            pages = int(self.amount.text) // 10    #格子輸入爬幾頁（因為一頁有十篇，除以10表示要爬幾頁）
        except Exception:
            self.pop_up.open()      #如果輸入非數字
            return
        my_crawler = PTTCrawler()
        my_sss = my_crawler.crawl(self.board.text, pages)   #文字出現頻率
        my_crawler.generate_cloud(my_sss)   #生成圖片

#啟動爬蟲程式
class TestApp(App):
    def build(self):
        self.title = "Word Cloud Generator"
        return UserInterface()


if __name__ == "__main__":
    TestApp().run()